---
title: "Critères d'acceptation des rapports d'incident"
metaTitle: "Critères d'acceptation des rapports d'incident"
metaDescription: "Quelles sont les raisons pour lesquelles les rapports d'incidents sont ou ne sont pas acceptés dans la base de données des incidents AI ?"
slug: "/research/1-criteria"
aiTranslated: true
---
# Qu'est-ce qu'un incident IA ?

La base de données des incidents d'IA contient des enregistrements d'incidents d'IA. Un « incident d'IA » est une situation dans laquelle les systèmes d'IA ont _causé_, ou _presque causé_, _des dommages réels_. En appliquant cette définition, notez qu'elle est censée être large. Si, après une enquête approfondie, vous n'êtes pas sûr qu'un incident soit admissible, faites un excès de signalement et soumettez-le.

# "IA"

Pour nos besoins, « IA » signifie la capacité des machines à exécuter des fonctions généralement considérées comme nécessitant l'intelligence humaine, telles que le raisonnement, la reconnaissance de modèles ou la compréhension du langage naturel. L'IA inclut, mais sans s'y limiter, l'apprentissage automatique - un ensemble de techniques par lesquelles un système informatique apprend à effectuer une tâche en reconnaissant des modèles dans les données et en déduisant des règles de décision, plutôt qu'au moyen d'instructions explicites.

# "Système IA"

Par « systèmes d'IA », nous entendons des technologies et des processus dans lesquels l'IA joue un rôle significatif. Ces systèmes peuvent également inclure des composants qui n'impliquent pas d'intelligence artificielle, tels que des composants mécaniques.

> **Exemples :** Une voiture autonome ; logiciel de reconnaissance faciale; Google Translate; un algorithme de notation de crédit.

Les algorithmes qui ne sont pas traditionnellement considérés comme de l'IA peuvent être considérés comme un système d'IA lorsqu'un humain transfère le pouvoir de décision au système.

> **Exemple :** Un système hospitalier sélectionne des vaccins candidats en fonction d'une série de règles personnalisées dans un algorithme de boîte noire.

# "Causé"

Lorsque nous disons qu'un système d'IA a « causé » des dommages, nous voulons dire qu'il a joué un rôle important dans la chaîne d'événements qui ont conduit à des dommages. Le système d'IA n'a pas besoin d'être le seul facteur, ni même le facteur principal, responsable du préjudice. Mais cela devrait au moins être une cause "sans" - c'est-à-dire que si le système d'IA n'avait pas agi comme il l'a fait, le préjudice spécifique ne se serait pas produit.

Nous ne faisons aucune distinction entre les dommages accidentels et délibérés (c'est-à-dire l'utilisation malveillante de l'IA). Aux fins de la base de données des incidents d'IA, ce qui compte, c'est que le préjudice a été causé, et non s'il était intentionnel.

# "Presque causé"

Lorsque nous disons qu'un système d'IA a "presque causé" un dommage, nous voulons dire qu'il a joué un rôle important dans une chaîne d'événements qui auraient facilement pu causer un dommage, mais un facteur externe a empêché le dommage de se produire. Ce facteur externe devrait être indépendant du système d'IA et ne devrait pas avoir été mis en place spécifiquement pour prévenir le dommage en question.

> **Exemple :** Un robot industriel commence à devenir incontrôlable, mais un travailleur à proximité parvient à couper l'alimentation du robot avant que le robot ou les personnes à proximité ne soient blessés.

> **Contre-exemple :** Un robot industriel commence à devenir incontrôlable, mais son capteur de sécurité intégré détecte l'anomalie et arrête immédiatement le robot.

Encore une fois, le système d'IA n'a pas besoin d'être le seul facteur, ni même le facteur principal, dans la chaîne d'événements qui auraient pu causer des dommages. Mais cela devrait au moins être une cause "sans" - c'est-à-dire que si le système d'IA n'avait pas agi comme il l'a fait, il n'y aurait eu aucune chance significative que le mal se produise.

# "Dommage dans le monde réel"

Nous avons une définition large du "préjudice dans le monde réel". Cela inclut, mais n'est pas limité à :

* Atteinte à la santé/sécurité physique
* Préjudice psychologique
* Préjudice financier
* Atteinte à la propriété physique
* Atteinte aux biens immatériels (par exemple, vol de propriété intellectuelle, atteinte à la réputation d'une entreprise)
* Atteinte aux systèmes sociaux ou politiques (par exemple, ingérence dans les élections, perte de confiance dans les autorités)
* Atteinte aux libertés civiles (par exemple, emprisonnement injustifié ou autre sanction, censure)

Il n'est pas nécessaire que les préjudices soient graves pour répondre à cette définition ; un incident entraînant des dépenses ou des inconvénients mineurs et faciles à réparer compte toujours comme un préjudice pour nos besoins.

Dans certains cas, en particulier lorsqu'il s'agit de préjudices psychologiques ou autrement intangibles, des personnes raisonnables peuvent ne pas être d'accord sur la question de savoir si un préjudice s'est réellement produit (ou a failli se produire). Les contributeurs doivent utiliser leur meilleur jugement, pécher par excès de trouver un préjudice lorsqu'il existe un argument plausible qu'un préjudice s'est produit.

# Qu'est-ce qui n'est pas un incident d'IA ?

Plusieurs situations courantes ne répondent _pas_ aux critères ci-dessus et ne doivent pas être enregistrées en tant qu'incidents dans la base de données des incidents AI. Ceux-ci inclus:

| Type                                                                                                                                                  | Exemple                                                                                                                                                                                                                    |
|-------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Systèmes d'IA fonctionnant mal dans des environnements contrôlés ou de test, y compris des environnements simulés                                                     | [De légères modifications des panneaux de signalisation peuvent complètement tromper les algorithmes d'apprentissage automatique](https://spectrum.ieee.org/cars-that-think/transportation/sensors/slight-street-sign-modifications-can-fool-machine-learning-algorithms) |
| Études documentant les failles théoriques ou conceptuelles de la technologie de l'IA                                                                                  | [Les réseaux de neurones profonds sont facilement trompés : prédictions à haute confiance pour les images non reconnaissables](https://www.cv-foundation.org/openaccess/content_cvpr_2015/html/Nguyen_Deep_Neural_Networks_2015_CVPR_paper.html)          |
| Expériences de pensée et exemples hypothétiques                                                                                                         | ["... l'intelligence générale artificielle, même conçue avec compétence et sans malveillance, pourrait finalement détruire l'humanité"](https://www.lesswrong.com/tag/paperclip-maximizer)                                             |
| Le développement ou le déploiement de produits d'IA qui semblent susceptibles de causer des dommages, mais il n'y a pas de rapports de situations spécifiques dans lesquelles des dommages ont été causés | [Les réseaux de neurones profonds sont plus précis que les humains pour détecter l'orientation sexuelle à partir d'images faciales](https://osf.io/zn79k/)                                                                                             |
| Discussions sur les grands types d'incidents d'IA ou de comportements d'IA nuisibles                                                                                    | [Comment une poignée d'entreprises technologiques contrôlent des milliards d'esprits chaque jour](https://www.ted.com/talks/tristan_harris_how_a_handful_of_tech_companies_control_billions_of_minds_every_day)                                        |
| Marketing trompeur des produits d'IA                                                                                                                   | [Le mystère de Zach, l'IA miraculeuse, suite : tout devient plus terrible](https://thespinoff.co.nz/the-best-of/09-03-2018/the-mystery-of-zach-the-miracle-ai-continued-it-all-just-gets-terribler/)                      |

Bien sûr, il existe de nombreux autres types de situations qui ne sont pas considérées comme des incidents, mais ce sont quelques-unes des plus courantes qui ont tendance à confondre les collaborateurs AIID.



