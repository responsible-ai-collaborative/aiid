---
title: "À propos de"
metaTitle: "Les détails concernant qui et pourquoi la base de données"
metaDescription: "Qui sont les personnes et les organisations qui développent l'AIID et pourquoi le développons-nous ?"
slug: "/about"
aiTranslated: true
---

import Leaderboards from '../../src/components/landing/Leaderboards';
import Sponsors from '../../src/components/landing/Sponsors';

## Pourquoi les "Incidents d'IA" ?

Les systèmes intelligents sont actuellement sujets à des défaillances imprévues et souvent dangereuses lorsqu'ils sont déployés dans le monde réel. Tout comme le secteur des transports avant lui (par exemple, [FAA](https://www.faa.gov/data_research/accident_incident/) et [FARS](https://www.nhtsa.gov/research-data/fatality-analysis-reporting-system-fars)) et plus récemment les [systèmes informatiques](https://cve.mitre.org/cve/), les systèmes intelligents nécessitent un référentiel des problèmes rencontrés dans le monde réel afin que les futurs chercheurs et développeurs puissent atténuer ou éviter la répétition de mauvais résultats.

## Qu'est-ce qu'un incident ?

L'ensemble initial de plus de 1 000 rapports d'incidents a été intentionnellement large par nature. Les exemples actuels incluent :

* Une *voiture autonome* [tue un piéton](/cite/4)
* Un *algorithme de trading* provoque un "[flash crash](/cite/28)" du marché où des milliards de dollars sont transférés entre les parties
* Un *système de reconnaissance faciale* [provoque l'arrestation d'une personne innocente](/cite/74)

Vous êtes invités à [explorer les incidents collectés à ce jour](/apps/discover), [consulter la liste complète](/summaries/incidents), et [soumettre](/apps/submit) des rapports d'incidents supplémentaires. Les chercheurs sont invités à examiner notre [définition de travail des incidents d'IA](/research/1-criteria).

## Utilisateurs actuels et futurs

La base de données est un produit de données et une collection d'applications en constante évolution.

* **Utilisateurs actuels** incluent les architectes système, les développeurs de produits industriels, les responsables des relations publiques, les [chercheurs](/research), les chercheurs en politique publique et le grand public. Ces utilisateurs sont invités à utiliser l'application [Discover](/apps/discover) pour découvrir de manière proactive comment les systèmes intelligents récemment déployés ont produit des résultats inattendus dans le monde réel. Ce faisant, ils peuvent éviter de commettre des erreurs similaires à l'avenir.
* **Utilisations futures** [évolueront](/research/2-roadmap) grâce aux contributions de code de la communauté [open source](https://github.com/responsible-ai-collaborative/aiid), y compris des [résumés](/about_apps/) de base de données supplémentaires et des [taxonomies](/research/2-roadmap).

## Quand devriez-vous signaler un incident ?

En cas de doute sur la qualification d'un événement comme incident, veuillez [le soumettre](/apps/submit) ! Ce projet vise à converger vers des [critères partagés](/research/1-criteria) pour l'ingestion d'incidents grâce à l'exploration des incidents candidats soumis par la communauté élargie.

## Conseil d'administration

La base de données d'incidents est gérée de manière participative par des personnes et des organisations contribuant au code, à la recherche et aux impacts plus larges. Si vous souhaitez participer à la gouvernance du projet, veuillez nous [contacter](/contact) et inclure votre contribution prévue à la base de données des incidents d'IA.

**Membres votants**

* **[Patrick Hall](https://www.hallresearch.ai/team-patrick-hall/):** Patrick était récemment scientifique principal chez bnh.ai, un cabinet d'avocats basé à Washington D.C. spécialisé dans l'IA et l'analyse de données. Patrick est également professeur adjoint à la George Washington University School of Business. Avant de co-fonder bnh.ai, Patrick a dirigé les efforts d'IA responsable chez la société de logiciels d'apprentissage automatique H2O.ai, où son travail a abouti à l'une des premières solutions commerciales au monde pour l'apprentissage automatique explicable et équitable. Entre autres écrits académiques et médiatiques technologiques, Patrick est l'auteur principal de livres électroniques populaires sur l'apprentissage automatique explicable et responsable. Patrick a étudié la chimie computationnelle à l'Université de l'Illinois avant d'obtenir son diplôme de l'Institute for Advanced Analytics de la North Carolina State University.  
**Contributions :** Patrick est un contributeur [principal](https://incidentdatabase.ai/summaries/leaderboard) de rapports d'incidents à la base de données des incidents d'IA et fournit un leadership stratégique au conseil.

* **[Heather Frase](https://www.linkedin.com/in/heather-frase/):** Heather Frase, PhD est Senior Fellow au [Center for Security and Emerging Technology](https://cset.georgetown.edu/) (CSET) de Georgetown, où elle travaille sur l'évaluation de l'IA. Elle sert également de conseillère non rémunérée au projet Open Loop de Meta, apportant son expertise sur la mise en œuvre du cadre de gestion des risques de l'IA du National Institute for Standards and Technology. Avant de rejoindre le CSET, Heather a passé huit ans à fournir un soutien en analyse de données, modélisation computationnelle, apprentissage automatique (ML) et intelligence artificielle (IA) pour des contrats de renseignement, de défense et fédéraux. De plus, Heather a passé 14 ans à l'Institute for Defense Analyses (IDA), soutenant le Director, Operational Test and Evaluation (DOT&E). À l'IDA, elle a dirigé des équipes de recherche analytique pour appliquer une expertise scientifique, technologique et statistique afin de développer des métriques de données et des plans de collecte pour les tests opérationnels des principaux systèmes de défense, analyser les données de test et produire des évaluations de l'efficacité et de l'adéquation opérationnelles. Elle détient un doctorat en science des matériaux du California Institute of Technology et un BS en physique de l'Université de Miami à Oxford, Ohio.  
**Contributions :** Heather développe la recherche sur les incidents d'IA en plus de sa supervision de la [taxonomie CSET](https://incidentdatabase.ai/taxonomies/cset).

* **[Kristian J. Hammond](https://www.mccormick.northwestern.edu/research-faculty/directory/profiles/hammond-kristian.html):** Kris Hammond est le professeur Bill et Cathy Osborn d'informatique à l'Université Northwestern et le co-fondateur de la société d'intelligence artificielle Narrative Science, récemment acquise par Salesforce. Il est également responsable facultaire de l'initiative CS + X de Northwestern, explorant comment la pensée computationnelle peut être utilisée pour transformer des domaines tels que le droit, la médecine, l'éducation et les affaires. Il est directeur du Master of Science in Artificial Intelligence (MSAI) de Northwestern. Plus récemment, le Dr Hammond a fondé le Center for Advancing Safety in Machine Intelligence (CASMI), un centre de recherche financé par Underwriter's Laboratories. CASMI se concentre sur l'opérationnalisation de la conception et de l'évaluation des systèmes d'IA du point de vue de leur impact sur la vie et la sécurité humaines.
**Contributions :** Kris développe un projet collaboratif centré sur les études de cas d'incidents.

**Conseil émérite**

Les membres émérites du conseil sont ceux qui se sont particulièrement distingués dans leur service au Responsible AI Collaborative. Ils n'occupent aucune position de gouvernance au sein de l'organisation.

* **[Sean McGregor](https://seanbmcgregor.com/):** Sean McGregor a fondé le projet de base de données des incidents d'IA et a récemment rejoint le [Digital Safety Research Institute](https://ul.org/research/digital-safety) en tant que directeur fondateur. Avant de démarrer le Responsible AI Collaborative, Sean a quitté un poste d'architecte d'apprentissage automatique chez la startup d'accélérateurs neuronaux Syntiant pour pouvoir se concentrer à plein temps sur l'assurance des systèmes intelligents. Le travail du Dr McGregor couvre les accélérateurs neuronaux pour l'inférence économe en énergie, l'apprentissage profond pour la parole et l'héliophysique, et l'apprentissage par renforcement pour la politique de suppression des incendies de forêt. En dehors de son travail rémunéré, Sean a organisé une série d'ateliers lors de grandes conférences académiques sur l'IA sur le thème de "l'IA pour le bien" et cherche maintenant à réaliser en toute sécurité les avantages de l'IA en reliant les enregistrements d'incidents d'IA aux programmes de test d'IA.

* **[Helen Toner](https://cset.georgetown.edu/staff/helen-toner/):** Helen Toner est directrice de la stratégie au Center for Security and Emerging Technology (CSET) de Georgetown. Elle a précédemment travaillé comme analyste de recherche senior chez Open Philanthropy, où elle conseillait les décideurs politiques et les bailleurs de fonds sur la politique et la stratégie de l'IA. Entre son travail chez Open Philanthropy et son arrivée au CSET, Helen a vécu à Pékin, étudiant l'écosystème chinois de l'IA en tant qu'affiliée de recherche du Center for the Governance of AI de l'Université d'Oxford. Helen a écrit pour Foreign Affairs et d'autres publications sur les implications de sécurité nationale de l'IA et de l'apprentissage automatique pour la Chine et les États-Unis, et a témoigné devant la Commission d'examen économique et de sécurité États-Unis-Chine. Elle est membre du conseil d'administration d'OpenAI. Helen détient une maîtrise en études de sécurité de Georgetown, ainsi qu'un BSc en génie chimique et un diplôme en langues de l'Université de Melbourne.

## Collaborateurs

**Responsible AI Collaborative :** Personnes qui servent l'organisation derrière la base de données des incidents d'IA.

* Les membres du conseil comprennent [Patrick Hall](https://www.hallresearch.ai/team-patrick-hall/), [Heather Frase](https://www.linkedin.com/in/heather-frase/), et [Kristian J. Hammond](https://www.mccormick.northwestern.edu/research-faculty/directory/profiles/hammond-kristian.html). Les biographies sont indiquées ci-dessus.
* [Sean McGregor](https://seanbmcgregor.com/about) est le directeur exécutif.
* [Daniel Atherton](https://www.linkedin.com/in/daniel-atherton-167819251/) est le rédacteur principal des incidents d'IA.

De nombreuses autres personnes et organisations sans affiliations formelles au Responsible AI Collaborative contribuent au projet.

**Digital Safety Research Institute (DSRI) :** Personnes affiliées au [DSRI](https://ul.org/research/digital-safety), qui fournit un soutien substantiel au programme AIID.

* [Kevin Paeth](https://www.linkedin.com/in/kevinpaeth/) est un responsable du DSRI
* [César Varela](https://github.com/cesarvarela) est ingénieur Full Stack
* [Luna McNulty](https://lmcnulty.me/) est ingénieure UX
* [Pablo Costa](https://www.linkedin.com/in/pablo-costa/) est ingénieur Full Stack
* [Clara Youdale Pinelli](https://www.linkedin.com/in/clarayoudale/) est ingénieure Front End
* [Sean McGregor](https://seanbmcgregor.com/) est directeur du DSRI

**Éditeurs d'incidents :** Personnes qui résolvent les soumissions d'incidents à la base de données et les maintiennent.

* [Daniel Atherton](https://www.linkedin.com/in/daniel-atherton-167819251/)
* [Sean McGregor](https://seanbmcgregor.com/)

De plus, [Zachary Arnold](https://cset.georgetown.edu/staff/zachary-arnold/) a apporté des contributions significatives aux [critères d'incident](/research/1-criteria).

**Éditeurs de taxonomie :** Organisations ou personnes qui ont contribué des [taxonomies](https://incidentdatabase.ai/taxonomies) à la base de données.

* [Center for Security and Emerging Technology](https://cset.georgetown.edu/) (CSET)
* [Nikiforos Pittaras](https://npit.github.io/) (GMF)

**Contributeurs Open Source :** Personnes qui ont contribué plus d'une pull request, des graphiques, du contenu de site ou un rapport de bug à la base de données des incidents d'IA.

* [Neama Dadkhahnikoo](https://www.linkedin.com/in/neama/): Neama a servi en tant que directrice exécutive bénévole et observatrice du conseil pour le Responsible AI Collaborative
* [Jingying Yang](https://www.linkedin.com/in/jingyingyang/) et [Dr. Christine Custis](https://www.su.edu/symposium/business-symposium-speakers/christine-custis/) ont contribué de manière significative aux premières étapes de l'AIID via leurs rôles avec le Partnership on AI.
* [Khoa Lam](https://www.linkedin.com/in/khoalklam/): A servi en tant qu'éditeur de données
* [Kate Perkins](https://www.linkedin.com/in/kateeperkins/): A servi en tant qu'éditrice de données
* [Scott Allen Cambo](https://www.scottallencambo.com/): Scott a précédemment servi en tant que directeur exécutif du Responsible AI Collaborative
* [Janet Boutilier Schwartz](https://www.linkedin.com/in/janet-boutilier-schwartz/): Janet a précédemment consulté sur les opérations et la stratégie avec le Responsible AI Collaborative.
* [Kit Harris](https://www.linkedin.com/in/kitharris/): Kit a servi en tant qu'observateur du conseil et a fourni des conseils stratégiques depuis sa position de conseiller en subventions.
* [Alex Muscă](https://github.com/alexmcode)
* [Chloe Kam](http://kamchy.com) A développé le logo AIID
* [JT McHorse](https://github.com/jt-mchorse)
* Seth Reid

**Contributeurs d'incidents :** Personnes qui ont contribué un grand nombre d'incidents à la base de données.

* [Roman Lutz (Max Planck Institute for Intelligent Systems, anciennement Microsoft)](/apps/discover?display=details&lang=en&page=1&submitters=Roman%20Lutz)
* [Patrick Hall (Burt and Hall LLP)](/apps/discover?display=details&lang=en&page=1&submitters=Patrick%20Hall%20%28BNH.ai%29)
* [Catherine Olsson (Google)](/apps/discover?display=details&lang=en&page=1&submitters=Catherine%20Olsson)
* [Roman Yampolskiy (University of Louisville)](/apps/discover?display=details&lang=en&page=1&submitters=Roman%20Yampolskiy)
* Sam Yoon (en tant que contractuel pour PAI, puis avec Deloitte Consulting, puis avec la Kennedy School of Government)

Les personnes suivantes ont collecté un grand nombre d'incidents en attente d'ingestion.

* Zachary Arnold, Helen Toner, Ingrid Dickinson, Thomas Giallella, et Nicolina Demakos (Center for Security and Emerging Technology, Georgetown)
* Lawrence Lee, Darlena Phuong Quyen Nguyen, Iftekhar Ahmed (UC Irvine)

Il existe une communauté croissante de personnes préoccupées par la collecte et la caractérisation des incidents d'IA, et nous encourageons tout le monde à contribuer au développement de ce système.

<Leaderboards />
<br />

<Sponsors />
