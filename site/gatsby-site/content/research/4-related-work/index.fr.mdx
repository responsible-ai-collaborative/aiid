---
title: "Related Work"
metaTitle: "Related Work"
metaDescription: "Related Work"
slug: "/research/4-related-work"
---

While formal AI incident research is relatively new, a number of people have been collecting what could be considered incidents. These include,

* [Awesome Machine Learning Interpretability: AI Incident Tracker](https://github.com/jphall663/awesome-machine-learning-interpretability/blob/master/README.md#ai-incident-tracker)
* [AI and Algorithimic Incidents and Controversies of Charlie Pownall](https://charliepownall.com/ai-algorithimic-incident-controversy-database/)
* [Map of Helpful and Harmful AI](https://map.ai-global.org/)

If you have an incident resource that could be added here, please [contact](/contact) us.

The following publications have been indexed by Google scholar as referencing the database itself, rather than solely individual incidents. Please contact us if your reference is missing.

## 2022 (through September 12th)

* [Braga, Juliao, et al. "Project for the Development of a Paper on Algorithm and Data Governance." (2022).](https://osf.io/sr7kt/download) ([Original Portuguese](https://osf.io/xcpsd/download)).
* [NIST. Risk Management Playbook. 2022](https://pages.nist.gov/AIRMF/)  
* [Shneiderman, Ben. Human-Centered AI. Oxford University Press, 2022.](https://www.amazon.com/Human-Centered-AI-Ben-Shneiderman/dp/0192845292)  
* [Schwartz, Reva, et al. "Towards a Standard for Identifying and Managing Bias in Artificial Intelligence." (2022).](https://tsapps.nist.gov/publication/get_pdf.cfm?pub_id=934464)  
* [McGrath, Quintin et al. An Enterprise Risk Management Framework to Design Pro-Ethical AI Solutions." University of South Florida. (2022](https://www.usf.edu/business/documents/desrist/paper_65.pdf)).  
* [Nor, Ahmad Kamal Mohd, et al. "Abnormality Detection and Failure Prediction Using Explainable Bayesian Deep Learning: Methodology and Case Study of Real-World Gas Turbine Anomalies." (2022).](https://www.mdpi.com/2227-7390/10/4/554/pdf)  
* [Xie, Xuan, Kristian Kersting, and Daniel Neider. "Neuro-Symbolic Verification of Deep Neural Networks." arXiv preprint arXiv:2203.00938 (2022).](https://arxiv.org/pdf/2203.00938)  
* [Hundt, Andrew, et al. "Robots Enact Malignant Stereotypes." 2022 ACM Conference on Fairness, Accountability, and Transparency. 2022.](https://dl.acm.org/doi/fullHtml/10.1145/3531146.3533138)  
* [Tidjon, Lionel Nganyewou, and Foutse Khomh. "Threat Assessment in Machine Learning based Systems." arXiv preprint arXiv:2207.00091 (2022).](https://arxiv.org/pdf/2207.00091)  
* [Naja, Iman, et al. "Using Knowledge Graphs to Unlock Practical Collection, Integration, and Audit of AI Accountability Information." IEEE Access 10 (2022): 74383-74411.](https://ieeexplore.ieee.org/iel7/6287639/9668973/09815594.pdf)  
* [Cinà, Antonio Emanuele, et al. "Wild Patterns Reloaded: A Survey of Machine Learning Security against Training Data Poisoning." arXiv preprint arXiv:2205.01992 (2022).](https://arxiv.org/pdf/2205.01992)  
* [Schröder, Tim, and Michael Schulz. "Monitoring machine learning models: A categorization of challenges and methods." Data Science and Management (2022).](https://www.sciencedirect.com/science/article/pii/S2666764922000303)  
* [Corea, Francesco, et al. "A principle-based approach to AI: the case for European Union and Italy." AI & SOCIETY (2022): 1-15.](https://link.springer.com/article/10.1007/s00146-022-01453-8)  
* [Carmichael, Zachariah, and Walter J. Scheirer. "Unfooling Perturbation-Based Post Hoc Explainers." arXiv preprint arXiv:2205.14772 (2022).](https://arxiv.org/pdf/2205.14772)  
* [Wei, Mengyi, and Zhixuan Zhou. "AI Ethics Issues in Real World: Evidence from AI Incident Database." arXiv preprint arXiv:2206.07635 (2022).](https://arxiv.org/pdf/2206.07635)  
* [Petersen, Eike, et al. "Responsible and Regulatory Conform Machine Learning for Medicine: A Survey of Challenges and Solutions." IEEE Access (2022).](https://ieeexplore.ieee.org/iel7/6287639/9668973/09783196.pdf)  
* [Karunagaran, Surya, Ana Lucic, and Christine Custis. "XAI Toolsheet: Towards A Documentation Framework for XAI Tools."](https://a-lucic.github.io/talks/xai_pai_toolsheets.pdf)  
* [Paudel, Shreyasha, and Aatiz Ghimire. "AI Ethics Survey in Nepal."](https://www.naamii.org.np/wp-content/uploads/2021/11/AI-Ethics-Survey-Report.pdf)  
* [Ferguson, Ryan. "Transform Your Risk Processes Using Neural Networks."](https://www.firm.fm/wp-content/uploads/2022/05/Papers-Round-Table-AI-April-2022.pdf)  
* [Fujitsu Corporation. "AI Ethics Impact Assessment Casebook," 2022](https://www.fujitsu.com/global/documents/about/research/technology/aiethics/fujitsu-AIethics-case_en.pdf)  
* [Shneiderman, Ben and Du, Mengnan. "Human-Centered AI: Tools" 2022](https://hcai.site/tools/)  
* [Salih, Salih. "Understanding Machine Learning Interpretability." Medium. 2022](https://towardsdatascience.com/understanding-machine-learning-interpretability-168fd7562a1a)  
* [Garner, Carrie. "Creating Transformative and Trustworthy AI Systems Requires a Community Effort." Software Engineering Institute. 2022](https://insights.sei.cmu.edu/blog/creating-transformative-and-trustworthy-ai-systems-requires-a-community-effort/)  
* [Weissinger, Laurin, AI, Complexity, and Regulation (February 14, 2022). The Oxford Handbook of AI Governance](https://academic.oup.com/edited-volume/41989)

## 2021

* [Arnold, Z., Toner, H., CSET Policy. "AI Accidents: An Emerging Threat." (2021).](https://cset.georgetown.edu/wp-content/uploads/CSET-AI-Accidents-An-Emerging-Threat.pdf)  
* [Aliman, Nadisha-Marie, Leon Kester, and Roman Yampolskiy. "Transdisciplinary AI Observatory—Retrospective Analyses and Future-Oriented Contradistinctions." Philosophies 6.1 (2021): 6.](https://www.mdpi.com/2409-9287/6/1/6/pdf)  
* [Falco, Gregory, and Leilani H. Gilpin. "A stress testing framework for autonomous system verification and validation (v&v)." 2021 IEEE International Conference on Autonomous Systems (ICAS). IEEE, 2021.](https://www.researchgate.net/profile/Gregory-Falco/publication/352930787_A_Stress_Testing_Framework_For_Autonomous_System_Verification_And_Validation_VV/links/60e911fcb8c0d5588ce64ec5/A-Stress-Testing-Framework-For-Autonomous-System-Verification-And-Validation-V-V.pdf)  
* [Petersen, Eike, et al. "Responsible and Regulatory Conform Machine Learning for Medicine: A Survey of Technical Challenges and Solutions." arXiv preprint arXiv:2107.09546 (2021).](https://arxiv.org/pdf/2107.09546)  
* [John-Mathews, Jean-Marie. AI ethics in practice, challenges and limitations. Diss. Université Paris-Saclay, 2021.](https://tel.archives-ouvertes.fr/tel-03527232/document)  
* [Macrae, Carl. "Learning from the Failure of Autonomous and Intelligent Systems: Accidents, Safety and Sociotechnical Sources of Risk." Safety and Sociotechnical Sources of Risk (June 4, 2021) (2021).](https://nottingham-repository.worktribe.com/OutputFile/7164920)  
* [Hong, Matthew K., et al. "Planning for Natural Language Failures with the AI Playbook." Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems. 2021.](https://www.adamfourney.com/papers/hong_chi2021.pdf)  
* [Ruohonen, Jukka. "A Review of Product Safety Regulations in the European Union." arXiv preprint arXiv:2102.03679 (2021).](https://arxiv.org/pdf/2102.03679)  
* [Kalin, Josh, David Noever, and Matthew Ciolino. "A Modified Drake Equation for Assessing Adversarial Risk to Machine Learning Models." arXiv preprint arXiv:2103.02718 (2021).](https://arxiv.org/pdf/2103.02718)  
* [Aliman, Nadisha Marie, and Leon Kester. "Epistemic defenses against scientific and empirical adversarial AI attacks." CEUR Workshop Proceedings. Vol. 2916. CEUR WS, 2021.](https://dspace.library.uu.nl/bitstream/handle/1874/413353/paper_1.pdf?sequence=1)  
* [John-Mathews, Jean-Marie. L’Éthique de l’Intelligence Artificielle en Pratique. Enjeux et Limites. Diss. université Paris-Saclay, 2021.](https://www.theses.fr/2021UPASI015.pdf)  
* [Smith, Catherine. "Automating intellectual freedom: Artificial intelligence, bias, and the information landscape." IFLA Journal (2021): 03400352211057145](https://journals.sagepub.com/doi/abs/10.1177/03400352211057145)


If you have a scholarly work that should be added here, please [contact](/contact) us.
