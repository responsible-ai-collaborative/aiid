---
title: 'Resumen de incidentes de IA: 22 de noviembre'
metaTitle: 'Resumen de incidentes de IA: 22 de noviembre'
metaDescription: 'Recopilación de incidentes, desarrollos e ingestión de IA en noviembre de 2022, que proporciona un resumen digerible de los últimos incidentes e informes de AIID.'
date: '2022-12-15'
image: './images/aiid-november.webp'
author: 'Janet Schwartz & Khoa Lam'
slug: '/blog/incident-report-2022-november'
---

Bienvenido a la edición de este mes de The Monthly Roundup, un boletín informativo diseñado para brindarle un resumen digerible de los últimos incidentes e informes de la base de datos de incidentes de AI.

Tiempo estimado de lectura: 3 minutos

## 🗞️ Nuevos Incidentes

Incidentes emergentes que ocurrieron el mes pasado:

- Incidente n.° 399: [Se informa que el generador de artículos científicos de Meta AI produjo contenido inexacto y dañino](/cite/399)

  - ¿Qué pasó? Meta AI entrenó y alojó un generador de artículos científicos que a veces producía mala ciencia y prohibía consultas sobre temas y grupos que probablemente produzcan contenido ofensivo o dañino.
  - ¿Quien estaba involucrado? Meta AI, Meta y Facebook desarrollaron e implementaron un sistema de IA que dañó a los grupos minoritarios, Meta AI, Meta y Facebook.

- Incidente n.° 410: [KFC envió una promoción insensible de la Noche de los cristales rotos a través del sistema de detección de días festivos](/cite/410)

  - ¿Qué pasó? KFC citó un error en un sistema automatizado de detección de festividades que identificó el aniversario de la Kristallnacht y provocó una notificación automática insensible que promocionaba su pollo.
  - ¿Quien estaba involucrado? KFC desarrolló e implementó un sistema de inteligencia artificial que perjudicó al pueblo judío.

- Incidente n.° 411: [Cuentas chinas presuntamente enviaron spam a Twitter para ocultar noticias de protestas](/cite/411)

  - ¿Qué pasó? El algoritmo de alimentación de Twitter se vio inundado por contenido de cuentas en chino que supuestamente tenían como objetivo manipular y reducir la cobertura de las redes sociales sobre las protestas generalizadas contra las restricciones del coronavirus en China.
  - ¿Quien estaba involucrado? Twitter desarrolló e implementó un sistema de inteligencia artificial que perjudicó a los usuarios de Twitter y a Twitter.

- Incidente n.° 413: [Miles de respuestas incorrectas producidas por ChatGPT publicadas en Stack Overflow](/cite/413)

  - ¿Qué pasó? Miles de respuestas incorrectas producidas por ChatGPT de OpenAI se enviaron a Stack Overflow, lo que inundó el proceso de curación de calidad basado en voluntarios del sitio y perjudicó a los usuarios que buscaban respuestas correctas.
  - ¿Quien estaba involucrado? OpenAI desarrolló e implementó un sistema de inteligencia artificial que perjudicó a los usuarios de Stack Overflow y Stack Overflow.

## 📎 Nuevos Desarrollos

Incidentes más antiguos que tienen nuevos informes o actualizaciones.

<table>
  <tr>
    <th align="center">Incidente original</th>
    <th align="center">Nuevo(s) reporte(s)</th>
  </tr>
  <tr>
    <td className="align-top border-1 border-gray-200 px-4 py-2">
      <strong>Incidente #240</strong>:
      <a href="/cite/240">
        GitHub Copilot, infracción de derechos de autor y licencias de código abierto
      </a>
    </td>
    <td className="align-top border-1 border-gray-200 px-4 py-2">
      <ul>
        <li>
          Litigio de GitHub Copilot –
          <a href="https://githubcopilotlitigation.com">githubcopilotlitigation.com</a>,
          <em>Nov 3, 2022</em>
        </li>
      </ul>
    </td>
  </tr>
  <tr>
    <td className="align-top border-1 border-gray-200 px-4 py-2">
      <strong>Incidente #376</strong>:
      <a href="/cite/376">
        El algoritmo de RealPage elevó los precios de alquiler, supuestamente de manera artificial
      </a>
    </td>
    <td className="align-top border-1 border-gray-200 px-4 py-2">
      <ul>
        <li>
          El Departamento de Justicia ha abierto una investigación sobre RealPage -
          <a href="https://www.propublica.org/article/yieldstar-realpage-rent-doj-investigation-antitrust">
            ProPublica
          </a>
          , <em>Nov 23, 2022</em>
        </li>
      </ul>
    </td>
  </tr>
</table>

## 🗄 De los archivos

En cada edición, presentamos uno o más incidentes históricos que encontramos estimulantes.

Dado que ha habido mucha cobertura de noticias y discurso en las redes sociales sobre ChatGPT de OpenAI, echemos un vistazo a algunos de los incidentes anteriores relacionados con los chatbots. Aquí hay algunos:

[Microsoft’s Tay](/cite/6) se lanzó el 23 de marzo de 2016 y se eliminó en 24 horas debido a múltiples tuits racistas, sexistas y antisemitas generados por el bot.

[Yandex’s Alice](/cite/58), un chatbot producido por una empresa de tecnología rusa, lanzado en octubre de 2017, comenzó a responder preguntas con respuestas racistas, pro-stalinistas y pro-violencia.

Se demostró que [Korean Chatbot Luda](/cite/106) utilizó un lenguaje despectivo e intolerante cuando se le preguntó acerca de las lesbianas, los negros y las personas con discapacidades.

La demostración del chatbot de [Meta’s BlenderBot 3](/cite/278) hizo comentarios antisemitas ofensivos.

Aunque la tecnología de IA generativa se ha vuelto mucho más avanzada y muy popular en solo unos pocos años, los problemas relacionados con el sesgo, la discriminación y el uso ético han sido problemas persistentes.

## 👇 Indagando más profundamente

- Todos los nuevos incidentes agregados a la base de datos en el último mes, agrupados por tema:

- Privacidad y vigilancia: [#386](/cite/386), [#387](/cite/387), [#395](/cite/395)
- Reconocimiento facial: [#388](/cite/388), [#391](/cite/391), [#409](/cite/409)
- Sesgo y discriminación: [#396](/cite/396), [#400](/cite/400), [#401](/cite/401), [#405](/cite/405), [ #407](/citar/407), [#410](/citar/410)
- Redes sociales: [#392](/cite/392), [#393](/cite/393), [#394](/cite/394), [#397](/cite/397), [# 399](/cite/399), [#406](/cite/406), [#408](/cite/408), [#411](/cite/411)
- GPT-3: [#402](/cite/402), [#413](/cite/413)
- Errores impactantes: [#403](/cite/403), [#404](/cite/404)
- Vehículos autónomos: [#389](/cite/389), [#398](/cite/398)
- Falsificaciones profundas: [#390](/cite/390)

- Explore grupos de incidentes similares en [Visualización espacial](/summaries/spatial)
- Consulte [Vista de tabla](/apps/incidents) para obtener una vista completa de todos los incidentes
- Obtenga información sobre presuntos desarrolladores, implementadores y partes perjudicadas en [Página de entidades](/entidades)

---

## 🦾 Apoya nuestros esfuerzos

¿Sigues leyendo? ¡Ayúdanos a cambiar el mundo para mejor!

1. Comparta este boletín en [LinkedIn](https://www.linkedin.com/company/responsible-ai-collaborative), [Twitter](https://twitter.com/IncidentsDB) y [Facebook](https ://www.facebook.com/IncidentsDB)
2. [Enviar](/apps/enviar) incidentes a la base de datos
3. [Contribuir](https://github.com/responsible-ai-collaborative/aiid) a la funcionalidad de la base de datos
