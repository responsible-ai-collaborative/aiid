---
title: 'ChatGPT Incidents and Issues'
metaTitle: 'ChatGPT Incidents and Issues'
metaDescription: 'This January, we received an influx of incidents involving OpenAI’s newly released ChatGPT. Here is our compilation of distinct ChatGPT incidents categorized by type of harm.'
date: '2023-03-07'
image: 'images/chatgpt-question-exclamation.png'
author: 'Khoa Lam'
slug: '/blog/chatgpt-incidents-and-issues'
---

_“ChatGPT, which has taken the internet by the storm recently, seems to be making it a lot easier for people to add to the chaos.” – [Why is Everyone Bashing ChatGPT?](https://analyticsindiamag.com/why-is-everyone-bashing-chatgpt/), Aparna Iyer, December 9, 2022_

This January, we received an influx of incidents involving OpenAI’s newly released ChatGPT. Here is our compilation of distinct ChatGPT incidents categorized by type of harm. 

**Academic dishonesty:** ChatGPT is based on a LLM released by OpenAI named GPT-3, which was previously abused by students to [cheat on assignments and exams](/cite/339/). Predictably, subsequent ChatGPT academic dishonesty incidents appeared in the database shortly after its release. Some students reported or even [documented themselves on social media](https://www.tiktok.com/@caleb_sorensen/video/7176768074023226670) using ChatGPT to complete assignments and final exams. These abuses in academic settings have resulted in counteractions, including [some schools banning the use of ChatGPT](https://www.nbcnews.com/tech/tech-news/new-york-city-public-schools-ban-chatgpt-devices-networks-rcna64446), and the emergence of ChatGPT-detection tools such as [GPTZero](https://gptzero.me) or [OpenAI’s AI Text Classifier](https://openai.com/blog/new-ai-classifier-for-indicating-ai-written-text/). However, [recent reports of these detection tools](/cite/466/) show that their error rates can be high.

**Malware development:** In another incident concerning ChatGPT abuse, cybersecurity researchers reported that [ChatGPT has been used to develop malicious software](https://incidentdatabase.ai/cite/443/) [of dubious quality](https://www.washingtonpost.com/politics/2023/01/26/yes-chatgpt-can-write-malware-code-not-well/). Though abuse of generative models to create malicious programs is not new, ChatGPT seems to enable a new group of cybercriminals who have little to no coding ability to potentially create malicious emails or code using only plain English.

**Jailbreaking with ease:** Despite OpenAI’s use of content filters to prevent tool misuse or abuse, [many users were able to easily bypass them](https://incidentdatabase.ai/cite/420/) by using simple “[prompt engineering](https://technical.ly/software-development/prompt-enginnering-chatgpt-dall-e-openai/).” On social media, users attempted to test the limits of these filters for a variety of purposes, such as [to detail ChatGPT’s risks](https://mobile.twitter.com/m1guelpf/status/1598203861294252033) or [to reveal hidden biases](https://www.newstatesman.com/quickfire/2022/12/chatgpt-shows-ai-racism-problem). Some of the more alarming cases include: tricking ChatGPT to provide instructions to [commit murder](https://twitter.com/JustAnkurBagchi/status/1598213680012795905) or [making bombs](https://www.thetimes.co.uk/article/chatgpt-bot-tricked-into-giving-bomb-making-instructions-say-developers-rvktrxqb5). Though it is not known whether harmful instructions have been carried out in the real world, the circumvention of controls has impacted the reputation of OpenAI.

**Content labeler labor issue:** On a related note, the development of these content filters involved [data annotators in Kenya](https://incidentdatabase.ai/cite/450) sorting through highly disturbing content with pay and labor practices reported as psychologically harmful and inadequately compensated. This incident reignites conversations about an ethical problem in AI development known as “[ghost work](https://marylgray.org/bio/on-demand/),” which concerns the intentionally-hidden human labor that powers these AI systems.

**Fake citations:** As shown on its site, ChatGPT is warned by OpenAI to “occasionally generate incorrect information.” This known limitation resulted in instances where, if prompted by users, [ChatGPT provided citations or citation URLs that do not exist](https://incidentdatabase.ai/cite/464). Interestingly, as shown in [this example](https://news.ycombinator.com/item?id=33841672), the tool generated very convincing-looking URLs, using known research-related domain names such as [https://link.springer.com/book](https://link.springer.com/book) or [https://www.researchgate.net](https://www.researchgate.net).

**Quality assurance:** Further downstream from model use, ChatGPT has run into problems with online platforms that rely on user submissions, such as [Stack Overflow](https://incidentdatabase.ai/cite/413)—a Q&A site for developers—and [Immunefi](https://incidentdatabase.ai/cite/452)—a white hat bug bounty platform. ChatGPT-generated answers or bug reports were deemed [not of high quality](https://www.coindesk.com/tech/2023/01/17/crypto-whitehat-platform-immunefi-banned-15-chatgpt-generated-bug-reports-heres-why/), and [overwhelmed the sites’ human-operated review or quality curation process](https://meta.stackoverflow.com/questions/421831/temporary-policy-chatgpt-is-banned) due to the number of submissions. In both incidents, the platform banned ChatGPT-produced submissions as a result.

Since this Blog Post was written, numerous [ChatGPT incidents](https://incidentdatabase.ai/apps/discover/?display=details&is_incident_report=false%7C%7Ctrue&page=1&s=chatgpt) have been added to the database. Additionally, please note that this list is not inclusive of all [_issues_ presented by ChatGPT](https://incidentdatabase.ai/apps/discover/?display=details&is_incident_report=false&page=1&s=chatgpt)—i.e., those where a real-world harm event has yet to occur. The AI Incident Database indexes and makes these "issues" [searchable](https://incidentdatabase.ai/apps/discover/?display=details&is_incident_report=false&page=1&s=chatgpt) separately from "incidents."

