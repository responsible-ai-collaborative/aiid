---
title: "Researcher Guide"
metaTitle: "Research incidents and/or Contribute to the Database"
metaDescription: "The database is in the process of opening to the world for research purposes"
---

# Defining an "AI Incident"

The commercial air travel industry owes much of its increasing safety to systematically analyzing and archiving past accidents and incidents within a shared database. In aviation, an accident is a case where substantial damage or loss of life occurs. Incidents are cases where the risk of an accident substantially increases. For example, when a small fire is quickly extinguished in a cockpit it is an "incident" but if the fire burns crew members in the course of being extinguished it is an "accident." The FAA [aviation database](https://www.faa.gov/data_research/accident_incident/) indexes flight log data and subsequent expert investigations into comprehensive examinations of both technological and human factors. In part due to this continual self-examination, air travel is one of the safest forms of travel. Decades of iterative improvements to safety systems and training have decreased fatalities [81 fold](https://theblogbyjavier.com/2020/01/02/aviation-safety-evolution-2019-update/) since 1970 when normalized for passenger miles.

Where the aviation industry has clear definitions, computer scientists and philosophers have long debated foundational definitions of artificial intelligence. In the absence of clear lines differentiating algorithms, intelligence, and the harms they may directly or indirectly cause, this database adopts an adaptive criteria for ingesting "incidents" where reports are accepted or rejected on the basis of a growing rule set.

More details about the acceptance criteria are available on the [criteria page](/research/1-criteria).

# Download the Index

The complete state of the database can be downloaded in weekly JSON, MongoDB, and CSV format [snapshots](/research/snapshots). We maintain these snapshots so you can create stable datasets for natural language processing research and academic analysis. Please [contact](/contact) to let us know what you are using the database for so we can list your work in the incident database and esnure your use case is not dropped from support.

# Citing the Database as a Whole

We invite you to cite:  
> McGregor, S. (2021) Preventing Repeated Real World AI Failures by Cataloging Incidents: The AI Incident Database. In Proceedings of the Thirty-Third Annual Conference on Innovative Applications of Artificial Intelligence (IAAI-21). Virtual Conference.

The [pre-print](https://arxiv.org/abs/2011.08512) is available on arXiv.

# Citing a Specific Incident

Every incident has its own suggested citation that credits both the submitter(s) of the incident and the editor(s) of the incident. The submitters are the people that submitted reports associated with the incident and their names are listed in the order in which their submissions were added to the AIID. Since reports can be added to an incident record through time, our suggested citation format includes the access date. You can find incident citations at `https://incidentdatabase.ai/cite/INSERT_NUMBER_HERE`.

# Submit Incidents #

**Add Single Incident Reports**

We currently accept incident reports in a quick add form on the [landing page](/) form, which does not credit submitters, and the [submit app](/apps/submit) which allows you to assign credit for the submission. Both types of submissions are a great help to preventing repeating history.

**Batch Adding**

If you have a large batch (i.e., more than 30) of incidents to add to the database, we can facilitate their import with a [CSV format](https://docs.google.com/spreadsheets/d/1FLguZeFziNKKtpx14TYZ5EjBuc3FCbUGmlMIdCDU6ak/edit?usp=sharing). The steps for getting many incidents batch added to the database are,

1. Copy the [example Google Sheets document](https://docs.google.com/spreadsheets/d/1FLguZeFziNKKtpx14TYZ5EjBuc3FCbUGmlMIdCDU6ak/edit?usp=sharing).
2. Columns B through J will be imported into the database and care
should be taken to maintain their format. The first column, "Incident
Number", will not be imported directly into the database but it is
used when resolving reports to existing incidents. Incident numbers
less than 10,000 are reserved for reports that should be associated
with an incident already in the DB. Values equal to or greater than 10k
are this spreadsheet's new incident counter. So if you want to import
multiple reports for a single incident, then the incident number
should be consistent across those reports.
3. Fill in the columns with the following information,
    * title is the title of the report.
    * author(s) contains comma separated values of the authors of the
    report. Some reports will not have authors, in which case place the publication name in this field.
    * submitter(s) contains comma separated values for the people or
    organizations submitting the report.
    * incident date is the date in which the incident likely happened.
    Many times a report will be written about an incident that happened
    months or years earlier. Sometimes there will be ambiguity in when the
    incident likely took place, in which case it is OK to input a best
    guess. For incidents that are ongoing or had multiple occurances, the
    incident date should be the earliest known date.
    * date published is the date the report was posted on the web. Some
    publications do not have a publication date, in which case you should use
    the Wayback machine in support of when the publication was definitely
    added by.
    * date downloaded is the date in which the content was copy/pasted
    into the text column.
    * report address is the URL for the report.
    * image address is the image preview for the report. Capturing this
    generally requires either viewing the HTML of the web page and
    capturing static asset paths, or using a web page parser like
    NewsPlease.
    * text is the text of the report copy pasted out of the website.
4. Email `batchadd -at- seanbmcgregor.com` with a link to your data.

# Related Work

While formal AI incident research is relatively new, a number of people have been collecting what could be considered incidents. These include,

* [Awesome Machine Learning Interpretability: AI Incident Tracker](https://github.com/jphall663/awesome-machine-learning-interpretability/blob/master/README.md#ai-incident-tracker)
* [AI and Algorithimic Incidents and Controversies of Charlie Pownall](https://charliepownall.com/ai-algorithimic-incident-controversy-database/)
* [Map of Helpful and Harmful AI](https://map.ai-global.org/)

If you have an incident resource that could be added here, please [contact](/contact) us.

The following publications have been indexed by Google scholar as referencing the database itself, rather than solely individual incidents. Please contact us if your reference is missing.

* [Arnold, Z., Toner, H., CSET Policy. "AI Accidents: An Emerging Threat." (2021).](https://cset.georgetown.edu/wp-content/uploads/CSET-AI-Accidents-An-Emerging-Threat.pdf)
* [Avin, Shahar, et al. "Filling gaps in trustworthy development of AI." Science 374.6573 (2021): 1327-1329.](https://arxiv.org/abs/2112.07773)
* [Schwartz, Reva, et al. "Towards a Standard for Identifying and Managing Bias in Artificial Intelligence." (2022).](https://tsapps.nist.gov/publication/get_pdf.cfm?pub_id=934464)
* [Aliman, Nadisha-Marie, Leon Kester, and Roman Yampolskiy. "Transdisciplinary AI Observatory—Retrospective Analyses and Future-Oriented Contradistinctions." Philosophies 6.1 (2021): 6.](https://www.mdpi.com/2409-9287/6/1/6/pdf)
* [Shneiderman, Ben. Human-Centered AI. Oxford University Press, 2022.](https://www.amazon.com/Human-Centered-AI-Ben-Shneiderman/dp/0192845292)
* [Nor, Ahmad Kamal Mohd, et al. "Abnormality Detection and Failure Prediction Using Explainable Bayesian Deep Learning: Methodology and Case Study of Real-World Gas Turbine Anomalies." (2022).](https://www.mdpi.com/2227-7390/10/4/554/pdf)
* [Falco, Gregory, and Leilani H. Gilpin. "A stress testing framework for autonomous system verification and validation (v&v)." 2021 IEEE International Conference on Autonomous Systems (ICAS). IEEE, 2021.](https://www.researchgate.net/profile/Gregory-Falco/publication/352930787_A_Stress_Testing_Framework_For_Autonomous_System_Verification_And_Validation_VV/links/60e911fcb8c0d5588ce64ec5/A-Stress-Testing-Framework-For-Autonomous-System-Verification-And-Validation-V-V.pdf)
* [Petersen, Eike, et al. "Responsible and Regulatory Conform Machine Learning for Medicine: A Survey of Technical Challenges and Solutions." arXiv preprint arXiv:2107.09546 (2021).](https://arxiv.org/pdf/2107.09546)
* [John-Mathews, Jean-Marie. AI ethics in practice, challenges and limitations. Diss. Université Paris-Saclay, 2021.](https://tel.archives-ouvertes.fr/tel-03527232/document)
* [Macrae, Carl. "Learning from the Failure of Autonomous and Intelligent Systems: Accidents, Safety and Sociotechnical Sources of Risk." Safety and Sociotechnical Sources of Risk (June 4, 2021) (2021).](https://nottingham-repository.worktribe.com/OutputFile/7164920)
* [Weissinger, Laurin, AI, Complexity, and Regulation (October 14, 2021). OUP Handbook on AI Governance, Forthcoming ](https://ssrn.com/abstract=3943968)
* [Hong, Matthew K., et al. "Planning for Natural Language Failures with the AI Playbook." Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems. 2021.](https://www.adamfourney.com/papers/hong_chi2021.pdf)
* [Ruohonen, Jukka. "A Review of Product Safety Regulations in the European Union." arXiv preprint arXiv:2102.03679 (2021).](https://arxiv.org/pdf/2102.03679)
* [Kalin, Josh, David Noever, and Matthew Ciolino. "A Modified Drake Equation for Assessing Adversarial Risk to Machine Learning Models." arXiv preprint arXiv:2103.02718 (2021).](https://arxiv.org/pdf/2103.02718)
* [Aliman, Nadisha Marie, and Leon Kester. "Epistemic defenses against scientific and empirical adversarial AI attacks." CEUR Workshop Proceedings. Vol. 2916. CEUR WS, 2021.](https://dspace.library.uu.nl/bitstream/handle/1874/413353/paper_1.pdf?sequence=1)
* [John-Mathews, Jean-Marie. L’Éthique de l’Intelligence Artificielle en Pratique. Enjeux et Limites. Diss. université Paris-Saclay, 2021.](https://www.theses.fr/2021UPASI015.pdf)
* [Xie, Xuan, Kristian Kersting, and Daniel Neider. "Neuro-Symbolic Verification of Deep Neural Networks." arXiv preprint arXiv:2203.00938 (2022).](https://arxiv.org/pdf/2203.00938)

If you have a scholarly work that should be added here, please [contact](/contact) us.
