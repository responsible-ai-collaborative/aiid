---
title: "À propos de"
metaTitle: "Les détails concernant qui et pourquoi la base de données"
metaDescription: "Qui sont les personnes et les organisations qui développent l'AIID et pourquoi le développons-nous ?"
slug: "/about"
aiTranslated: true
---

import Leaderboards from '../../src/components/landing/Leaderboards';
import Sponsors from '../../src/components/landing/Sponsors';

# Pourquoi "Incidents IA" ?

Les systèmes intelligents sont actuellement sujets à des pannes imprévues et souvent dangereuses lorsqu'ils sont déployés dans le monde réel. Tout comme le secteur des transports avant lui (par exemple, [FAA](https://www.faa.gov/data_research/accident_incident/) et [FARS](https://www.nhtsa.gov/research-data/fatality- analysis-reporting-system-fars)) et plus récemment [systèmes informatiques](https://cve.mitre.org/cve/), les systèmes intelligents nécessitent un référentiel des problèmes rencontrés dans le monde réel afin que les futurs chercheurs et développeurs puissent atténuer ou éviter les mauvais résultats répétés.

# Qu'est-ce qu'un incident ?

L'ensemble initial de plus de 1 000 rapports d'incidents a été intentionnellement large. Les exemples actuels incluent,

* Une *voiture autonome* [tue un piéton](/cite/4)
* Un *algorithme de trading* provoque un marché "[flash crash](/cite/28)" où des milliards de dollars sont transférés entre les parties
* Un *système de reconnaissance faciale* [provoque l'arrestation d'une personne innocente](/cite/74)

Vous êtes invité à [explorer les incidents collectés à ce jour](/apps/discover), [afficher la liste complète](/summaries/incidents) et [soumettre](/apps/submit) des rapports d'incident supplémentaires. Les chercheurs sont invités à consulter notre [définition de travail des incidents d'IA](/research/1-criteria).
# Utilisateurs actuels et futurs

La base de données est [en constante évolution](/research/2-roadmap) [produit de données](/develop) et [collection d'applications](/apps).

* **Les utilisateurs actuels** incluent les architectes système, les développeurs de produits industriels, les responsables des relations publiques, les [chercheurs](/recherche) et les chercheurs en politique publique. Ces utilisateurs sont invités à utiliser l'application [Discover](/apps/discover) pour découvrir de manière proactive comment les systèmes intelligents récemment déployés ont produit des résultats inattendus dans le monde réel. Ce faisant, ils peuvent éviter de commettre des erreurs similaires dans leur développement.
* **Les utilisations futures** vont [évoluer](/research/2-roadmap) grâce aux contributions de code de la communauté [open source](https://github.com/PartnershipOnAI/aiid), y compris une base de données supplémentaire [summaries] (/summaries) et [taxonomies](/research/2-roadmap).

# Quand devez-vous signaler un incident ?

En cas de doute sur la qualification d'un événement en tant qu'incident, veuillez le [soumettre](/apps/submit) ! Ce projet vise à converger vers une [définition partagée](/research/1-criteria) d'« incident d'IA » grâce à l'exploration des incidents candidats soumis par la communauté au sens large.
#Conseil d'administration

La base de données des incidents est gérée de manière participative par des personnes et des organisations qui contribuent au code, à la recherche et aux impacts plus larges. Si vous souhaitez participer à la gouvernance du projet, veuillez nous [contact](/contact) et inclure votre contribution prévue à la base de données des incidents d'IA.

**Membres votants**

* **[Helen Toner](https://cset.georgetown.edu/staff/helen-toner/):** Helen Toner est directrice de la stratégie au Centre pour la sécurité et les technologies émergentes (CSET) de Georgetown. Elle a précédemment travaillé en tant qu'analyste de recherche senior chez Open Philanthropy, où elle a conseillé les décideurs politiques et les bailleurs de fonds sur la politique et la stratégie d'IA. Entre son travail chez Open Philanthropy et son arrivée au CSET, Helen a vécu à Pékin, étudiant l'écosystème chinois de l'IA en tant que chercheur affilié au Centre pour la gouvernance de l'IA de l'Université d'Oxford. Helen a écrit pour les Affaires étrangères et d'autres médias sur les implications de l'IA et de l'apprentissage automatique en matière de sécurité nationale pour la Chine et les États-Unis, et a témoigné devant la Commission d'examen de l'économie et de la sécurité entre les États-Unis et la Chine. Elle est membre du conseil d'administration d'OpenAI. Helen est titulaire d'une maîtrise en études de sécurité de Georgetown, ainsi que d'une licence en génie chimique et d'un diplôme en langues de l'Université de Melbourne.
**Contributions :** [Recherche sur les incidents liés à l'IA](https://cset.georgetown.edu/publication/ai-accidents-an-emerging-threat/) et supervision de la [taxonomie CSET](https://incidentdatabase. ai/taxonomie/cset).

* **[Patrick Hall](https://business.gwu.edu/johnston-patrick-hall):** Patrick est scientifique principal chez bnh.ai, un cabinet d'avocats basé à D.C. spécialisé dans l'IA et l'analyse de données. Patrick est également professeur invité à la George Washington University School of Business. Avant de cofonder bnh.ai, Patrick a dirigé les efforts d'IA responsable au sein de la société de logiciels d'apprentissage automatique H2O.ai, où son travail a abouti à l'une des premières solutions commerciales au monde pour un apprentissage automatique explicable et équitable. Parmi d'autres écrits académiques et technologiques, Patrick est le principal auteur de livres électroniques populaires sur l'apprentissage automatique explicable et responsable. Patrick a étudié la chimie computationnelle à l'Université de l'Illinois avant d'être diplômé de l'Institute for Advanced Analytics de la North Carolina State University.
**Contributions :** Patrick est le [leader](https://incidentdatabase.ai/summaries/leaderboard) contributeur des rapports d'incidents au projet de base de données des incidents d'IA.

* **[Sean McGregor](https://seanbmcgregor.com/):** Sean McGregor a fondé le projet AI Incident Database et a récemment quitté un poste d'architecte d'apprentissage automatique au démarrage de l'accélérateur neuronal Syntiant afin qu'il puisse se concentrer sur l'assurance de systèmes intelligents à plein temps. Les travaux du Dr McGregor couvrent les accélérateurs neuronaux pour l'inférence écoénergétique, l'apprentissage en profondeur pour la parole et l'héliophysique, et l'apprentissage par renforcement pour la politique de suppression des incendies de forêt. En dehors de son travail rémunéré, Sean a organisé une série d'ateliers lors de grandes conférences universitaires sur l'IA sur le thème de "l'IA pour le bien" et développe actuellement une approche basée sur des incitations pour rendre l'IA plus sûre par le biais d'audits et d'assurances. **Contributions :** Soyez bénévoles en tant que responsable du projet et éditeur du projet AI Incident Database (AIID).

**Membres sans droit de vote**

* **[Neama Dadkhahnikoo](https://www.linkedin.com/in/neama/):** Neama Dadkhahnikoo est un expert en intelligence artificielle et en entrepreneuriat, avec plus de 15 ans d'expérience dans le développement technologique au sein de startups, non bénéfices et les grandes entreprises. Il est actuellement chef de produit pour Vertex AI, la plate-forme unifiée de Google Cloud pour l'apprentissage automatique de bout en bout. Auparavant, M. Dadkhahnikoo était directeur de l'IA et des opérations de données à la Fondation XPRIZE, CTO de CaregiversDirect (start-up d'IA pour les soins à domicile), co-fondateur et COO de Textpert (start-up d'IA pour la santé mentale) et consultant en startup. Il a commencé sa carrière en tant que développeur de logiciels pour The Boeing Company. M. Dadkhahnikoo est titulaire d'un MBA de UCLA Anderson ; une maîtrise en gestion de projet du Stevens Institute of Technology; et un BA en mathématiques appliquées et en informatique, avec une mineure en physique, de l'UC Berkeley.
**Contributions :** Neama se porte volontaire pour maintenir l'organisation de la Responsible AI Collaborative en dehors de son rôle de chef de produit chez Google.
* **[Kit Harris](https://www.linkedin.com/in/kitharris/):** Le kit mène des enquêtes sur les subventions et recherche des domaines prometteurs chez Longview Philanthropy. Il écrit également sur les recommandations et les recherches de Longview pour les philanthropes clients de Longview. Avant de se concentrer sur un travail philanthropique à fort impact, Kit a travaillé comme trader de dérivés de crédit avec J.P. Morgan. Pendant ce temps, il a fait don de la majorité de ses revenus à des œuvres caritatives à fort impact. Kit est titulaire d'un diplôme de premier cycle en mathématiques de l'Université d'Oxford.
**Contributions :** Kit sert d'observateur au conseil d'administration, fournit des conseils stratégiques et est le point de contact de l'IRAC à Longview.

# Collaborateurs

**Contributeurs Open Source :** Personnes ayant contribué à plusieurs requêtes d'extraction, graphiques, copies de site ou rapports de bogues à la base de données des incidents d'IA.

* [César Varela] (https://github.com/cesarvarela)
* [Alex Musca] (https://github.com/alexmcode)
* [Chloé Kam] (http://kamchy.com) (Logos AIID)
* [JT McHorse](https://github.com/jt-mchorse)
* Seth Reid

**Partenariat avec les membres du personnel d'AI :**
[Jingying Yang](https://www.linkedin.com/in/jingyingyang/) et [Dr. Christine Custis](https://www.su.edu/symposium/business-symposium-speakers/christine-custis/) a contribué de manière significative aux premières étapes de l'AIID.

**Éditeurs d'incidents :** personnes qui résolvent les incidents soumis à la base de données.

*[Sean McGregor](https://seanbmcgregor.com/)

**Editeurs de taxonomie :** Organisations ou personnes qui ont contribué [taxonomies](https://incidentdatabase.ai/taxonomies) à la base de données.

* [Centre pour la sécurité et les technologies émergentes] (https://cset.georgetown.edu/) (CSET)

**Contributeurs d'incidents :** personnes qui ont contribué à un grand nombre d'incidents dans la base de données.

* [Kate Perkins (Intel)](/apps/discover?display=details&lang=en&page=1&submitters=Kate%20Perkins)
* [Roman Lutz (Institut Max Planck pour les systèmes intelligents, anciennement Microsoft)](/apps/discover?display=details&lang=en&page=1&submitters=Roman%20Lutz)
* [Patrick Hall (Burt and Hall LLP)](/apps/discover?display=details&lang=en&page=1&submitters=Patrick%20Hall%20%28BNH.ai%29)
* [Catherine Olsson (Google)](/apps/discover?display=details&lang=fr&page=1&submitters=Catherine%20Olsson)
* [Roman Yampolskiy (Université de Louisville)](/apps/discover?display=details&lang=en&page=1&submitters=Roman%20Yampolskiy)
* Sam Yoon (en tant qu'entrepreneur de PAI, puis de Deloitte Consulting, puis de la Kennedy School of Government)

Les personnes suivantes ont collecté un grand nombre d'incidents en attente d'ingestion.

* Zachary Arnold, Helen Toner, Ingrid Dickinson, Thomas Giallella et Nicolina Demakos (Centre pour la sécurité et les technologies émergentes, Georgetown)
* Charlie Pownall via AI, référentiel d'incidents et de controverses algorithmiques et d'automatisation [(AIAAIC)](https://www.aiaaic.org/)
*Lawrence Lee, Darlena Phuong Quyen Nguyen, Iftekhar Ahmed (UC Irvine)

Il existe une communauté croissante de personnes concernées par la collecte et la caractérisation des incidents d'IA, et nous encourageons tout le monde à contribuer au développement de ce système.

<Leaderboards />
<br />

<Sponsors />