---
title: 'Deepfakes y seguridad infantil: encuesta y análisis de incidentes y respuestas de 2023'
metaTitle: 'Deepfakes y seguridad infantil: encuesta y análisis de incidentes y respuestas de 2023'
metaDescription: ""
date: '2024-01-09'
image: './images/image_doj.jpg'
author: 'Daniel Atherton'
slug: '/blog/deepfakes-and-child-safety'
aiTranslated: true
---

**AVISO LEGAL:** Esta publicación no es un consejo o comentario legal y no debe interpretarse como tal.

En 2023 se produjo un aumento de los materiales de abuso sexual infantil (CSAM) generados por IA, junto con procesamientos de los infractores, una variedad de intentos legislativos para combatir los deepfakes de IA dirigidos a menores y la [orden ejecutiva sobre inteligencia artificial de la administración Biden](https://www.whitehouse.gov/briefing-room/presidential-actions/2023/10/30/executive-order-on-the-safe-secure-and-trustworthy-development-and-use-of-artificial-intelligence/).

Los deepfakes se pueden clasificar en términos generales en dos categorías principales, cada una con su propio subconjunto relacionado con CSAM. La primera categoría incluye deepfakes de individuos reales, donde el daño predominante asociado con CSAM surge de la generación de pornografía deepfake con niños reales. La segunda categoría abarca los deepfakes en los que los sujetos son completamente virtuales pero convincentemente realistas. En esta categoría, las preocupaciones sobre CSAM están relacionadas principalmente con la creación de medios audiovisuales sintéticos inapropiados que representan niños virtuales. En conjunto, estas dos categorías taxonómicas demuestran las diversas formas preocupantes en que se puede emplear la tecnología deepfake, especialmente en la generación y proliferación de CSAM.

Este artículo proporciona una instantánea de parte del trabajo de la base de datos de incidentes de IA en el seguimiento de estos incidentes emergentes, junto con un estudio de algunas de las respuestas legislativas incipientes.

Tres casos judiciales específicos (uno de Corea del Sur, uno de Quebec y uno de Carolina del Norte) se centran en hombres que utilizaron inteligencia artificial para generar pornografía infantil ultrafalsa.

En [Corea del Sur](https://incidentdatabase.ai/cite/600), el acusado anónimo generó 360 imágenes de niños en situaciones sexuales. Durante el juicio, los fiscales enfatizaron que la definición legal de material de explotación sexual debería extenderse a representaciones que involucren a “humanos virtuales” y no limitarse a imágenes de niños reales. Argumentaron que con la llegada de la tecnología de “alto nivel”, como la inteligencia artificial, las imágenes lo suficientemente realistas como para parecerse a menores reales pueden constituir contenido sexualmente abusivo. Esta postura fue respaldada por el fallo del caso, alineándose con el punto de vista del fiscal sobre las capacidades tecnológicas modernas y sus implicaciones en la ley.

En el [caso Quebec](https://incidentdatabase.ai/cite/604), Steven Larouche, de Sherbrooke, fue condenado a más de tres años de prisión por producir pornografía infantil sintética utilizando tecnología deepfake. Admitió haber creado al menos siete vídeos superponiendo caras a otros cuerpos. Además, Larouche fue declarado culpable de posesión de una vasta colección de pornografía infantil, lo que le llevó a cuatro años y medio adicionales de prisión. El juez del tribunal provincial Benoit Gagnon señaló que este es el primer caso de explotación infantil relacionado con deepfake del país y expresó su preocupación por el posible uso indebido de esta tecnología para manipular imágenes de niños en las redes sociales.

El [caso de Carolina del Norte](https://incidentdatabase.ai/cite/605) involucró al psiquiatra infantil David Tatum. Entre sus otros cargos, la evidencia presentada en su juicio reveló que Tatum empleó IA para transformar imágenes vestidas de menores en material explícito, incluida la alteración de fotografías de un baile escolar y una celebración del primer día de clases utilizando una aplicación de IA basada en la web.

El análisis del discurso de los medios arroja una divergencia en los tipos de incidentes que surgen como resultado de esta tecnología. Los casos anteriores se alinean con informes de larga data sobre depredadores infantiles y sus sentencias, aunque con la presencia emergente de IA como parte del perfil criminal general. A continuación, dos casos importantes de diferente tipo demuestran la proliferación en las comunidades locales (ya sean ciudades o escuelas) del uso de imágenes de niñas en pornografía deepfake.

En [Almendralejo, España](https://incidentdatabase.ai/cite/610), se utilizó IA para crear y distribuir imágenes de niñas en las que aparecían desnudas. Según los informes, las imágenes, en las que aparecen varias chicas locales, se difundieron ampliamente, lo que dio lugar a una investigación policial. Las madres de las niñas concienciaron sobre el tema, preocupadas por la posibilidad de subir estas imágenes a sitios pornográficos. Los funcionarios regionales confirmaron que hay una investigación en curso y que se han identificado algunos sospechosos. Este incidente, descrito como violencia digital de género, generó una condena generalizada.

De manera similar, en [Westfield High School en Nueva Jersey](https://incidentdatabase.ai/cite/597), las imágenes pornográficas de estudiantes generadas por IA causaron una angustia significativa y provocaron una investigación. Dorota Mani, cuya hija Francesca fue atacada, expresó su preocupación por el impacto de la IA en los niños y presentó un informe policial, y los dos [aparecieron en CNN](https://www.cnn.com/2023/11/ 04/us/new-jersey-high-school-deepfake-porn/index.html) para discutir el incidente. La respuesta de la escuela puede ser vista con el tiempo como uno de los primeros estudios de caso importantes para responder a estos incidentes. Casi al mismo tiempo que Westfield, la escuela secundaria Issaquah en el estado de Washington estaba experimentando [su propio problema similar](https://incidentdatabase.ai/cite/617) con deepfakes.

La mayoría de los ejemplos analizados anteriormente parecen implicar principalmente la apropiación indebida de retratos de niños reales. Por el contrario, el caso de Corea del Sur es un ejemplo de tecnología utilizada para generar medios audiovisuales sintéticos que representan a niños que no existen en la vida real, lo que también presenta problemas importantes.

Recientemente, [un incidente](https://incidentdatabase.ai/cite/576/) salió a la luz cuando un usuario de LinkedIn informó que una aplicación de inteligencia artificial conocida como PicSo_ai estaba generando contenido inapropiado, de manera alarmante, enfocado en "niñas". Este no fue un caso aislado. Una búsqueda en Instagram recomendó autosugerencias preocupantes relacionadas con imágenes de niñas menores de edad generadas por IA, marcadas como “populares”. Este descubrimiento apunta hacia un patrón preocupante en el que la IA se está explotando en una zona gris y donde, según [una investigación reciente del *Wall Street Journal*](https://www.wsj.com/tech/meta-instagram-video-algorithm-children-adult-sexual-content-72874155?mod=hp_lead_pos7), el algoritmo Reels de Instagram recomendaba contenido sexualmente sugerente y explícito relacionado con niños y adultos a los usuarios que seguían a jóvenes influencers. Esto generó importantes preocupaciones sobre la moderación y la seguridad del contenido de la plataforma, y los anuncios de las principales marcas aparecen junto a dicho contenido. A pesar de los esfuerzos de Meta por implementar herramientas de seguridad, los desafíos en la curación algorítmica de contenido y la seguridad digital en las redes sociales son persistentes.

*The Wall Street Journal* también [informado recientemente](https://www.wsj.com/tech/facebook-and-instagram-steer-predators-to-children-new-mexico-attorney-general-alleges-in-demand-b76a5b04?mod=Searchresults_pos1&page=1) sobre el Fiscal General de Nuevo México que presentó una demanda contra Meta, alegando que los algoritmos de Facebook e Instagram dirigieron a los depredadores y al contenido pornográfico a cuentas de prueba de temas menores. La investigación involucró imágenes de niños ficticios generadas por IA, lo que resultó en que las cuentas recibieran mensajes explícitos y proposiciones sexuales. La demanda afirma que las plataformas de Meta se han convertido en un mercado para depredadores y critica su falta de protección a los usuarios menores de edad, citando varios casos penales de explotación a través de estas plataformas. En este caso, fueron los investigadores quienes generaron las imágenes de los menores ficticios, adaptando las nuevas tecnologías a antiguas técnicas en este ámbito específico de la aplicación de la ley.

Un [estudio reciente](https://stacks.stanford.edu/file/druid:kh752sm9123/ml_training_data_csam_report-2023-12-21.pdf) realizado por David Thiel del Stanford Internet Observatory detalla la presencia de CSAM en los datos de entrenamiento de Modelos generativos de aprendizaje automático, centrándose en el conjunto de datos LAION-5B utilizado para modelos como Difusión estable. A través de varios métodos, incluidos los clasificadores PhotoDNA y ML, Thiel identificó numerosos casos nuevos y conocidos de CSAM en el conjunto de datos. Los hallazgos son oportunos ya que muestran la necesidad de prácticas de capacitación de modelos y curación de datos más rigurosas para evitar la perpetuación de contenido dañino, en línea con las preocupaciones planteadas por los incidentes en las plataformas de redes sociales y enfatizando la importancia de un mayor desarrollo responsable de la IA en este sentido. frente.

Actualmente, los esfuerzos legales para abordar los deepfakes de CSAM han sido reactivos y poco sistemáticos. Sin embargo, la orden ejecutiva del presidente Biden sobre la IA tiene como objetivo establecer normas estrictas para prevenir el uso indebido de la IA, centrándose en la seguridad nacional y la seguridad individual, lo que implica autenticar el contenido digital y etiquetar los medios sintéticos, especialmente para proteger a los niños de los daños provocados por la IA. Los desarrolladores deben compartir los resultados de las pruebas de seguridad de la IA antes de su uso público, centrándose en problemas como la creación de CSAM. La orden dirige el desarrollo de estándares para la autenticación de contenido y la detección de IA, y aborda el CSAM generado por IA y las imágenes sexualizadas no consensuales. Antes de la orden ejecutiva, [U.S. Los fiscales generales instaron al Congreso](https://www.scag.gov/media/pvehppkm/54-state-ags-urge-study-of-ai-and-harmful-impacts-on-children.pdf) a investigar el papel de la IA. en la explotación infantil, enfatizando la necesidad de una legislación integral sobre privacidad de datos.

Hasta el momento, [no existe ningún proyecto de ley federal general](https://www.nbcnews.com/news/us-news/little-recourse-teens-girls-victimized-ai-deepfake-nudes-rcna126399), pero se han realizado esfuerzos. (por ejemplo, [Ley de Responsabilidad H.R.3230 DEEP FAKES](https://www.congress.gov/bill/116th-congress/house-bill/3230)). A continuación se muestran cuatro ejemplos de legislación a nivel estatal:

* [California, AB 602, Sección 1708.86](https://pluralpolicy.com/app/legislative-tracking/bill/details/state-ca-20192020-ab602/202869): Modifica la ley existente no solo para permitir acciones contra aquellos que crean y comparten material sexualmente explícito sin consentimiento, pero también enfatiza específicamente acciones legales contra personas que distribuyen dicho material que ellos no crearon, siempre que sean conscientes de que la persona representada no dio su consentimiento para su creación. Esta faceta del proyecto de ley es importante ya que extiende la responsabilidad a quienes propagan contenido explícito no consensuado, independientemente de su papel en su creación original.

* [Florida, CS/CS/SB 1798](https://www.flsenate.gov/Session/Bill/2022/1798/?Tab=BillText): Aborda los delitos sexuales que involucran imágenes alteradas, específicamente deepfakes. Penaliza la promoción de representaciones sexuales alteradas sin consentimiento, definiéndolas como delitos graves de tercer grado. El proyecto de ley amplía la definición de “pornografía infantil” para incluir imágenes modificadas digitalmente que representan a menores en conducta sexual. También aumenta las penas por ciberacoso sexual y contacto sexual con animales, incluida la posesión de muñecas sexuales con apariencia de niños. Se hacen exenciones para ciertas entidades, como los medios de comunicación y las fuerzas del orden, bajo condiciones específicas. El proyecto de ley enfatiza la protección de los menores y abordar las tendencias emergentes de explotación sexual digital.

* [Illinois, Proyecto de Ley 2123](https://legiscan.com/IL/bill/HB2123/2023): Redefine “imagen sexual” en la Ley de Remedios Civiles para la Difusión No Consensual de Imágenes Sexuales Privadas para incluir imágenes que muestren desnudez o conducta, ya sea real o alterada digitalmente. Permite a las personas que aparecen en estas imágenes emprender acciones legales contra quienes comparten o amenazan con compartir estas imágenes, especialmente cuando no hay consentimiento. La enmienda especifica que revelar la alteración digital de una imagen no es una defensa. También elimina ciertas responsabilidades de los servicios informáticos interactivos y aclara que agregar un mensaje político a tales imágenes no las convierte en un asunto de preocupación pública. Además, los tribunales pueden dictar órdenes para detener la difusión de estas imágenes.

* [Virginia, Código 18.2-386.2](https://law.lis.virginia.gov/vacode/title18.2/chapter8/section18.2-386.2/): convierte en un delito menor de Clase 1 compartir o vender desnudos maliciosamente o imágenes sexualmente explícitas de alguien sin su consentimiento, especialmente si se hacen para acosar, coaccionar o intimidar. Esto incluye imágenes que han sido alteradas digitalmente para representar a una persona. Los proveedores de servicios de Internet no se hacen responsables del contenido compartido por otros. Se pueden emprender acciones legales donde se produjo el hecho ilícito o donde se manipuló la imagen. También pueden aplicarse otros cargos legales.

En la base de datos de incidentes de IA (AIID), hemos estado catalogando e investigando con preocupación estos incidentes recientes de CSAM. Como toda tecnología, la IA generativa plantea riesgos y oportunidades, y los riesgos para los niños en este caso son graves. Si desea unirse a nosotros en nuestra misión de documentar incidentes de IA con el objetivo de aprender de los errores del pasado para mitigar riesgos futuros, puede conectarse con nosotros a través de nuestra página de [contacto](https://incidentdatabase.ai/contact/) . Agradecemos los envíos que informen sobre todos y cada uno de los incidentes de IA utilizando nuestra página [envío](https://incidentdatabase.ai/apps/submit/); sin embargo, tenga en cuenta que, si bien rastreamos y analizamos activamente las tendencias e incidentes de CSAM, el AIID no es el destino directo para informar sobre CSAM real. Dirija la denuncia de CSAM al Departamento de Justicia [Sección de Obscenidad y Explotación Infantil](https://www.justice.gov/criminal/criminal-ceos/report-violations).